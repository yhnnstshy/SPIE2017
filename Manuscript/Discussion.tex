In general, with more training iterations $CAD_{DL}$'s performance increases suggesting that the architecture is learning discriminating features (Figure 5). However, the learning progress slows down with increasing number of epochs reaching its maximum performance at epoch 5 (Table 2). Due to a limited data-set size, the CAD achieves its optimal performance after a small number of epochs. In addition, with more training the number of false-positives decreases, but at the same time, the number of true-positives decreases --albeit at a lower rate-- evidenced by the ROC curves that shift toward the y-axis while decreasing in detection rate for high false-positive rates (Figure 6). This suggests that the architecture is overfitting the training data resulting in missed detection of tumors found in the test-set that may be underrepresented in the training-set. A small data-set makes it likely that the training-set excludes some prostate lesion types that may be found in the test-set. Since CNNs learn important features from the data, the CAD will be unable to detect such lesion types because of the lack of corresponding features.

$CAD_{DL}$ further shows potential for clinical application (Figure 7). It outperforms a published prostate-CAD with a margin of roughly 10\% detection rate for 10 false-positives per patient despite a smaller training-set. If trained on a more extensive data-set, $CAD_{DL}$ has the potential to perform at a higher detection rate for clinically significant lesions. Furthermore, the generated prediction maps have high probability values making them less ambiguous to interpret (Figure 8). False-positive predictions such as benign prostatic hyperplasia (BPH) (Figure 9) and those arising from artifacts in the image (Figure 11) exhibit high probability signatures as well; however, whether the CAD should be penalized for these types of erroneous predictions is debatable. Furthermore, the $CAD_{DL}$ tends to miss small lesions (Figure 10) or overestimate the size of a lesion (Figure 9). This may be as a result of a lack of small-tumor example types in the training-set, or large receptive field sizes for the various layers (Figure 4) that are unable to capture relevant features for small lesions.

A small study cohort is one limitation in our study. We investigate $CAD_{DL}$ performance over multiple epochs instead of finding the best performing model using a validation-set. With a large enough data-set, it will be possible to split the data into training, validation, and testing sets to thoroughly characterize the performance of $CAD_{DL}$. A second limitation in our study is the loss of information during image compression to satisfy the constraints of using publicly available CNN architectures. Even though we apply a histogram equalization algorithm to convert the medical images from a 12-16 bit to 8 bit images, there remains a small amount of information loss. Despite these limitation, we hope that our experimental methods and results elucidate the challenges that accompany the application of DCNN architectures to medical images and ways to circumvent such challenges. 

As a future study, we plan to investigate a weakly supervised learning scheme that can use the more ubiquitous database types containing biopsy points and histopathology images for training. Acquiring a large enough data-set with ground-truth tumor annotations is difficult because it required the expertise of experienced healthcare providers, who are preoccupied with patient care. Furthermore, our chosen CNN architecture can be modified for multi-label classification, which will allow us to identify lesions by severity in addition to detecting them. Lastly, we will attempt to develop an automatic segmentation tool for tumor size prediction. 
